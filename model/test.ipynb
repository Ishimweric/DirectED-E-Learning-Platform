{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c76cc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.figma.com/resource-library/graphic-design-principles/\"\n",
    "response = requests.get(url)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "soup = BeautifulSoup(response.text, \"html\")\n",
    "\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "  pattern = re.compile('<.*?>')\n",
    "  return pattern.sub(r'', text)\n",
    "cleaned_text = remove_html_tags(soup.get_text())\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "  file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7cafc26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_qa_to_llama_format(input_file, output_file):\n",
    "    # Specify UTF-8 encoding explicitly\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    converted_data = []\n",
    "    for item in data:\n",
    "        formatted_text = f\"<s>[INST] {item['human']} [/INST] {item['assistant']}</s>\"\n",
    "        converted_data.append({\"text\": formatted_text})\n",
    "    \n",
    "    # Also specify UTF-8 for output\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Usage\n",
    "convert_qa_to_llama_format('content.json', 'llama.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0fea2952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b47e948b27491b968b1bdc7d3f6fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARANJA\\Desktop\\Module_4_assessment\\DirectED-E-Learning-Platform\\model\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KARANJA\\.cache\\huggingface\\hub\\datasets--sahil2801--CodeAlpaca-20k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df62b1e882344b8db53f2c1a5beeaccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_alpaca_20k.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0abcb5250d54dcca5483c6f8c9ffe92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652b35701662470e9cb8a1fd5d879954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/21 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 20022\n",
      "{'output': 'arr = [2, 4, 6, 8, 10]', 'instruction': 'Create an array of length 5 which contains all even numbers between 1 and 10.', 'input': ''}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "dataset = load_dataset(\"sahil2801/CodeAlpaca-20k\")\n",
    "\n",
    "# Save as JSON\n",
    "dataset['train'].to_json('CodeAlpaca-20k.json')\n",
    "\n",
    "# Or access the data directly\n",
    "train_data = dataset['train']\n",
    "print(f\"Number of examples: {len(train_data)}\")\n",
    "print(train_data[0])  # First example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "61d990bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 20022 examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_codealpaca_to_llama(input_file, output_file):\n",
    "    converted_data = []\n",
    "    \n",
    "    # Read JSONL file (one JSON object per line)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                item = json.loads(line)\n",
    "                \n",
    "                instruction = item['instruction'].strip()\n",
    "                input_text = item.get('input', '').strip()\n",
    "                output_text = item['output'].strip()\n",
    "                \n",
    "                # Combine instruction and input if input exists\n",
    "                if input_text and input_text not in ['', '< noinput >', '<noinput>']:\n",
    "                    user_prompt = f\"{instruction}\\n\\n{input_text}\"\n",
    "                else:\n",
    "                    user_prompt = instruction\n",
    "                \n",
    "                # Format for Llama 7B Chat\n",
    "                formatted_text = f\"<s>[INST] {user_prompt} [/INST] {output_text}</s>\"\n",
    "                converted_data.append({\"text\": formatted_text})\n",
    "    \n",
    "    # Save as proper JSON array\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Converted {len(converted_data)} examples\")\n",
    "\n",
    "# Usage\n",
    "convert_codealpaca_to_llama('CodeAlpaca-20k.json', 'llama_codealpaca.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "65f90cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building message lookup...\n",
      "Converting conversations...\n",
      "Processed 1000 conversations...\n",
      "Processed 2000 conversations...\n",
      "Processed 3000 conversations...\n",
      "Processed 4000 conversations...\n",
      "Processed 5000 conversations...\n",
      "Processed 6000 conversations...\n",
      "Processed 7000 conversations...\n",
      "Processed 8000 conversations...\n",
      "Processed 9000 conversations...\n",
      "Processed 10000 conversations...\n",
      "Saving 10000 conversations...\n",
      "Done! Saved 10000 conversations to llama_oasst2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "def convert_oasst2_to_llama_fast(output_file=\"llama_oasst2.json\", max_examples=10000):\n",
    "   print(\"Loading dataset...\")\n",
    "   dataset = load_dataset(\"OpenAssistant/oasst2\")\n",
    "   \n",
    "   converted_data = []\n",
    "   count = 0\n",
    "   \n",
    "   # Create a simple lookup for faster access\n",
    "   print(\"Building message lookup...\")\n",
    "   msg_lookup = {item['message_id']: item for item in dataset['train']}\n",
    "   \n",
    "   print(\"Converting conversations...\")\n",
    "   for item in dataset['train']:\n",
    "       if count >= max_examples:\n",
    "           break\n",
    "           \n",
    "       # Only process assistant messages that are direct replies to prompters\n",
    "       if (item['role'] == 'assistant' and \n",
    "           item['parent_id'] and \n",
    "           item.get('review_result', False) == True):  # Only quality responses\n",
    "           \n",
    "           parent = msg_lookup.get(item['parent_id'])\n",
    "           \n",
    "           if (parent and \n",
    "               parent['role'] == 'prompter' and \n",
    "               item['lang'] == 'en'):  # English only\n",
    "               \n",
    "               user_text = parent['text'].strip()\n",
    "               assistant_text = item['text'].strip()\n",
    "               \n",
    "               # Skip if either text is too short/long\n",
    "               if 10 < len(user_text) < 2000 and 10 < len(assistant_text) < 4000:\n",
    "                   conversation = f\"<s>[INST] {user_text} [/INST] {assistant_text}</s>\"\n",
    "                   converted_data.append({\"text\": conversation})\n",
    "                   count += 1\n",
    "                   \n",
    "                   if count % 1000 == 0:\n",
    "                       print(f\"Processed {count} conversations...\")\n",
    "   \n",
    "   print(f\"Saving {len(converted_data)} conversations...\")\n",
    "   with open(output_file, 'w', encoding='utf-8') as f:\n",
    "       json.dump(converted_data, f, indent=2, ensure_ascii=False)\n",
    "   \n",
    "   print(f\"Done! Saved {len(converted_data)} conversations to {output_file}\")\n",
    "   return len(converted_data)\n",
    "\n",
    "# Usage - should complete in under 2 minutes\n",
    "count = convert_oasst2_to_llama_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fdb42132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying LangChainDatasets/langchain-howto-queries...\n",
      "Sample keys: ['inputs']\n",
      "Found 0 examples from LangChainDatasets/langchain-howto-queries\n",
      "Trying antonioibars/langchain-docs...\n",
      "Sample keys: ['id', 'text', 'source']\n",
      "Failed to load antonioibars/langchain-docs: 'source'\n",
      "Trying clue2solve/langchain-additional-resources...\n",
      "Sample keys: ['id', 'text', 'source']\n",
      "Failed to load clue2solve/langchain-additional-resources: 'source'\n",
      "Total saved: 108 LangChain examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "def download_langchain_alternatives():\n",
    "    \"\"\"Try multiple LangChain datasets\"\"\"\n",
    "    \n",
    "    datasets_to_try = [\n",
    "        \"LangChainDatasets/langchain-howto-queries\",\n",
    "        \"antonioibars/langchain-docs\", \n",
    "        \"clue2solve/langchain-additional-resources\"\n",
    "    ]\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for dataset_name in datasets_to_try:\n",
    "        try:\n",
    "            print(f\"Trying {dataset_name}...\")\n",
    "            dataset = load_dataset(dataset_name)\n",
    "            \n",
    "            # Check dataset structure\n",
    "            if dataset['train']:\n",
    "                sample = dataset['train'][0]\n",
    "                print(f\"Sample keys: {list(sample.keys())}\")\n",
    "                \n",
    "                for item in dataset['train']:\n",
    "                    # Adapt based on actual structure\n",
    "                    text_content = \"\"\n",
    "                    \n",
    "                    if 'text' in item:\n",
    "                        text_content = item['text']\n",
    "                    elif 'content' in item:\n",
    "                        text_content = item['content']\n",
    "                    elif 'question' in item and 'answer' in item:\n",
    "                        text_content = f\"Question: {item['question']}\\nAnswer: {item['answer']}\"\n",
    "                    elif 'instruction' in item and 'output' in item:\n",
    "                        text_content = f\"<s>[INST] {item['instruction']} [/INST] {item['output']}</s>\"\n",
    "                    \n",
    "                    if text_content:\n",
    "                        # Convert to Llama format if not already\n",
    "                        if '[INST]' not in text_content:\n",
    "                            # Create instruction from content\n",
    "                            instruction = f\"Explain this LangChain concept\"\n",
    "                            text_content = f\"<s>[INST] {instruction} [/INST] {text_content}</s>\"\n",
    "                        \n",
    "                        all_data.append({\n",
    "                            \"text\": text_content                    \n",
    "                        })\n",
    "                \n",
    "                print(f\"Found {len([x for x in all_data if x['source'] == dataset_name])} examples from {dataset_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {dataset_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # If still no data, create from LangChain documentation scraping\n",
    "    if not all_data:\n",
    "        print(\"No datasets found, creating synthetic LangChain examples...\")\n",
    "        all_data = create_synthetic_langchain_data()\n",
    "    \n",
    "    # Save results\n",
    "    with open(\"langchain_combined.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Total saved: {len(all_data)} LangChain examples\")\n",
    "    return len(all_data)\n",
    "\n",
    "def create_synthetic_langchain_data():\n",
    "    \"\"\"Create basic LangChain Q&A pairs\"\"\"\n",
    "    \n",
    "    langchain_qa = [\n",
    "        {\n",
    "            \"question\": \"How do I create a simple LLM chain in LangChain?\",\n",
    "            \"answer\": \"You can create an LLM chain using: from langchain.llms import OpenAI; from langchain.chains import LLMChain; from langchain.prompts import PromptTemplate; llm = OpenAI(); prompt = PromptTemplate(template='Question: {question}', input_variables=['question']); chain = LLMChain(llm=llm, prompt=prompt)\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is a PromptTemplate in LangChain?\",\n",
    "            \"answer\": \"A PromptTemplate is a class that helps you create prompts with variables. It allows you to define a template with placeholders like {variable_name} and then fill them dynamically with actual values.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do I use vector stores in LangChain?\",\n",
    "            \"answer\": \"Vector stores in LangChain allow you to store and search embeddings. You can use them with: from langchain.vectorstores import FAISS; from langchain.embeddings import OpenAIEmbeddings; embeddings = OpenAIEmbeddings(); vectorstore = FAISS.from_texts(texts, embeddings)\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are LangChain agents?\",\n",
    "            \"answer\": \"LangChain agents are entities that can use tools to interact with the world. They combine LLMs with tools like search engines, calculators, or APIs to perform complex tasks that require multiple steps.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do I implement RAG with LangChain?\",\n",
    "            \"answer\": \"To implement RAG (Retrieval Augmented Generation): 1) Create embeddings of your documents, 2) Store them in a vector database, 3) Use a retriever to find relevant documents, 4) Combine retrieved context with your query in a prompt to the LLM.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    converted_data = []\n",
    "    for qa in langchain_qa:\n",
    "        text = f\"<s>[INST] {qa['question']} [/INST] {qa['answer']}</s>\"\n",
    "        converted_data.append({\n",
    "            \"text\": text,\n",
    "            \"source\": \"synthetic\"\n",
    "        })\n",
    "    \n",
    "    return converted_data\n",
    "\n",
    "# Run the fixed script\n",
    "count = download_langchain_alternatives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "452f85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load LangChain documentation dataset\n",
    "dataset = load_dataset(\"hudsongeorge/langchain-docs\")\n",
    "\n",
    "# Load LangChain how-to queries\n",
    "howto_dataset = load_dataset(\"LangChainDatasets/langchain-howto-queries\")\n",
    "\n",
    "# Convert to Llama format for fine-tuning\n",
    "def convert_langchain_docs_to_llama(dataset):\n",
    "    converted = []\n",
    "    for item in dataset['train']:\n",
    "        if 'question' in item and 'answer' in item:\n",
    "            text = f\"<s>[INST] {item['question']} [/INST] {item['answer']}</s>\"\n",
    "            converted.append({\"text\": text})\n",
    "    with open(\"lang.json\", 'w', encoding='utf-8') as f:\n",
    "       json.dump(converted, f, indent=2, ensure_ascii=False)\n",
    "converted = convert_langchain_docs_to_llama(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "87e4ec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AI principles datasets...\n",
      "Loading airoboros dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4518fb47b98f4a729dee879fbd678567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARANJA\\Desktop\\Module_4_assessment\\DirectED-E-Learning-Platform\\model\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KARANJA\\.cache\\huggingface\\hub\\datasets--jondurbin--airoboros-gpt4-1.4.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68787a21344c451ebec75e5190103e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "as_conversations.json:   0%|          | 0.00/50.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35ed9fc60034051b342a06c337e89ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "instructions.jsonl:   0%|          | 0.00/45.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528613965e6649bd986e10b89dccdfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with airoboros: An error occurred while generating the dataset\n",
      "\n",
      "All the data files must have the same columns, but at some point there are 4 new columns ({'category', 'question_id', 'instruction', 'response'}) and 2 missing columns ({'conversations', 'id'}).\n",
      "\n",
      "This happened while the json dataset builder was generating data using\n",
      "\n",
      "hf://datasets/jondurbin/airoboros-gpt4-1.4.1/instructions.jsonl (at revision 433c04038d724bf29a193bc3c1a48b600cc417a1)\n",
      "\n",
      "Please either edit the data files to have matching columns, or separate them into different configurations (see docs at https://hf.co/docs/hub/datasets-manual-configuration#multiple-configurations)\n",
      "Loading OpenAssistant for AI topics...\n",
      "Found 7804 AI conversations from OASST2\n",
      "Total saved: 7804 AI principles examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7804"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "def download_ai_principles_dataset(output_file=\"ai_principles.json\"):\n",
    "    print(\"Loading AI principles datasets...\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Dataset 1: Airoboros (contains AI reasoning)\n",
    "    try:\n",
    "        print(\"Loading airoboros dataset...\")\n",
    "        dataset = load_dataset(\"jondurbin/airoboros-gpt4-1.4.1\")\n",
    "        \n",
    "        for item in dataset['train']:\n",
    "            if any(keyword in item.get('instruction', '').lower() \n",
    "                  for keyword in ['ai', 'artificial intelligence', 'machine learning', 'reasoning', 'logic']):\n",
    "                \n",
    "                instruction = item['instruction'].strip()\n",
    "                response = item['response'].strip()\n",
    "                \n",
    "                text = f\"<s>[INST] {instruction} [/INST] {response}</s>\"\n",
    "                all_data.append({\n",
    "                    \"text\": text,\n",
    "                    \"source\": \"airoboros\",\n",
    "                    \"category\": \"ai_principles\"\n",
    "                })\n",
    "        \n",
    "        print(f\"Found {len([x for x in all_data if x['source'] == 'airoboros'])} AI-related examples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with airoboros: {e}\")\n",
    "    \n",
    "    # Dataset 2: Filter OASST2 for AI topics\n",
    "    try:\n",
    "        print(\"Loading OpenAssistant for AI topics...\")\n",
    "        dataset = load_dataset(\"OpenAssistant/oasst2\")\n",
    "        msg_lookup = {item['message_id']: item for item in dataset['train']}\n",
    "        \n",
    "        ai_keywords = ['artificial intelligence', 'machine learning', 'ai', 'neural network', \n",
    "                      'deep learning', 'algorithm', 'data science', 'model training']\n",
    "        \n",
    "        for item in dataset['train']:\n",
    "            if (item['role'] == 'assistant' and \n",
    "                item['parent_id'] and \n",
    "                item['lang'] == 'en' and\n",
    "                item.get('review_result', False)):\n",
    "                \n",
    "                parent = msg_lookup.get(item['parent_id'])\n",
    "                if parent and parent['role'] == 'prompter':\n",
    "                    \n",
    "                    user_text = parent['text'].lower()\n",
    "                    if any(keyword in user_text for keyword in ai_keywords):\n",
    "                        \n",
    "                        conversation = f\"<s>[INST] {parent['text'].strip()} [/INST] {item['text'].strip()}</s>\"\n",
    "                        all_data.append({\n",
    "                            \"text\": conversation,\n",
    "                            \"source\": \"oasst2\",\n",
    "                            \"category\": \"ai_functionality\"\n",
    "                        })\n",
    "        \n",
    "        print(f\"Found {len([x for x in all_data if x['source'] == 'oasst2'])} AI conversations from OASST2\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with OASST2: {e}\")\n",
    "    \n",
    "    # Save combined dataset\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Total saved: {len(all_data)} AI principles examples\")\n",
    "    return len(all_data)\n",
    "\n",
    "# Usage\n",
    "download_ai_principles_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b7ceea0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e42bdb928647f4b6440c8ffc240cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARANJA\\Desktop\\Module_4_assessment\\DirectED-E-Learning-Platform\\model\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KARANJA\\.cache\\huggingface\\hub\\datasets--Pavithrars--AI_dataset. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e97ff256ac46b9af8f3f785565e798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AI_Tutor.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241dab17e45f487b88234fe66b9519e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44cbfb12703432398254cc2519fd38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved llama_ai_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load dataset\n",
    "dataset = load_dataset(\"Pavithrars/AI_dataset\")\n",
    "\n",
    "# 2. Conversion function: dataset -> LLaMA-2 chat template\n",
    "def to_llama_format(example):\n",
    "    instruction = example.get(\"Question\") or example.get(\"instruction\") or \"Explain AI.\"\n",
    "    output = example.get(\"Answer\") or example.get(\"output\") or \"Artificial Intelligence is...\"\n",
    "    \n",
    "    # LLaMA 2 Chat format\n",
    "    text = f\"<s>[INST] <<SYS>>\\nYou are a helpful assistant that explains how AI works.\\n<</SYS>>\\n\\n{instruction} [/INST] {output} </s>\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "# 3. Apply conversion\n",
    "train_dataset = dataset[\"train\"].map(to_llama_format)\n",
    "\n",
    "# 4. Save as JSON (list of objects)\n",
    "data_list = [row for row in train_dataset]\n",
    "with open(\"llama_ai_dataset.json\", \"w\") as f:\n",
    "    json.dump(data_list, f, indent=2)\n",
    "\n",
    "print(\"Saved llama_ai_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d0249e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset size: 173 examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load your original JSON\n",
    "with open(\"llama_ai_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Keep only the \"text\" field for fine-tuning\n",
    "cleaned = [{\"text\": item[\"text\"]} for item in data if \"text\" in item]\n",
    "\n",
    "# Save cleaned dataset\n",
    "with open(\"llama_ai_dataset_clean.json\", \"w\") as f:\n",
    "    json.dump(cleaned, f, indent=2)\n",
    "\n",
    "print(f\"Cleaned dataset size: {len(cleaned)} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b35b3b4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'lucadiliello/How_AI_Works' on the Hub (LocalEntryNotFoundError)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load dataset (replace with the one you actually picked)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlucadiliello/How_AI_Works\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m records = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Extract fields safely\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KARANJA\\Desktop\\Module_4_assessment\\DirectED-E-Learning-Platform\\model\\venv\\Lib\\site-packages\\datasets\\load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KARANJA\\Desktop\\Module_4_assessment\\DirectED-E-Learning-Platform\\model\\venv\\Lib\\site-packages\\datasets\\load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KARANJA\\Desktop\\Module_4_assessment\\DirectED-E-Learning-Platform\\model\\venv\\Lib\\site-packages\\datasets\\load.py:1031\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1027\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1028\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KARANJA\\Desktop\\Module_4_assessment\\DirectED-E-Learning-Platform\\model\\venv\\Lib\\site-packages\\datasets\\load.py:953\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m LocalEntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    946\u001b[39m         e.__cause__,\n\u001b[32m    947\u001b[39m         (\n\u001b[32m   (...)\u001b[39m\u001b[32m    951\u001b[39m         ),\n\u001b[32m    952\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    955\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Couldn't reach 'lucadiliello/How_AI_Works' on the Hub (LocalEntryNotFoundError)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset (replace with the one you actually picked)\n",
    "dataset = load_dataset(\"lucadiliello/How_AI_Works\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    # Extract fields safely\n",
    "    question = item.get(\"title\") or item.get(\"question\") or \"Explain this AI concept.\"\n",
    "    answer = item.get(\"document\") or item.get(\"content\") or item.get(\"summary\") or \"No answer available.\"\n",
    "\n",
    "    # Format in LLaMA style\n",
    "    text = f\"<s>[INST] <<SYS>>\\nYou are a helpful assistant that explains how AI works.\\n<</SYS>>\\n\\n{question} [/INST] {answer} </s>\"\n",
    "\n",
    "    records.append({\"text\": text})\n",
    "\n",
    "# Save as JSON (not JSONL)\n",
    "with open(\"llama_ai_dataset2.json\", \"w\") as f:\n",
    "    json.dump(records, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Saved {len(records)} examples in llama_ai_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d91e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Model in 4-bit (QLoRA)\n",
    "# -----------------------------\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype=\"float16\"\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # HF model repo\n",
    "\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # avoids pad errors\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Dataset\n",
    "# -----------------------------\n",
    "# Example: local JSON file in Colab\n",
    "dataset = load_dataset(\"json\", data_files=\"final.json\", split=\"train\")\n",
    "# Split the dataset into training and validation sets\n",
    "train_ds, val_ds = dataset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. LoRA Config\n",
    "# -----------------------------\n",
    "peft_config = LoraConfig(\n",
    "    r=32,                 # rank (bottleneck size)\n",
    "    lora_alpha=16,        # scaling factor\n",
    "    lora_dropout=0.05,    # dropout probability\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Training Arguments\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-llama7b-finetuned\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,      # Effective batch size = 4 * 4 = 16\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,  # Changed from 0 to save checkpoints\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,                          \n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=2            # Keep only 2 latest checkpoints\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Trainer (FIXED)\n",
    "# -----------------------------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,      # Fixed: use train_ds instead of dataset\n",
    "    eval_dataset=val_ds,         # Fixed: this was correct\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    # If your dataset has a specific text field, add:\n",
    "    # dataset_text_field=\"text\"  # Only add this if your JSON has a \"text\" field\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Train\n",
    "# -----------------------------\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Save Adapter\n",
    "# -----------------------------\n",
    "model.save_pretrained(\"./qlora-llama7b-adapter\")\n",
    "tokenizer.save_pretrained(\"./qlora-llama7b-adapter\")\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. AUTHENTICATE & UPLOAD DATA (CRITICAL FIX)\n",
    "# ---------------------------------------------------------------------------\n",
    "# Login to Hugging Face to download the gated Llama 2 model\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57980c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()  # pick llama_ai_dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b36058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U accelerate peft bitsandbytes transformers trl datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9a06e4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.figma.com/resource-library/portfolio-website-examples/\"\n",
    "response = requests.get(url)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "soup = BeautifulSoup(response.text, \"html\")\n",
    "\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "  pattern = re.compile('<.*?>')\n",
    "  return pattern.sub(r'', text)\n",
    "cleaned_text = remove_html_tags(soup.get_text())\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "  file.write(cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
